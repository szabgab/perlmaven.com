=title A Simple way to download many web pages using Perl
=timestamp 2015-04-12T00:30:01
=indexes LWP::Simple, HTTP::Tiny, wget, curl
=status show
=author szabgab
=comments_disqus_enable 1

=abstract start

There are plenty of choices when you need to fetch a page or two from the Internet.
We are going to see several simple examples using wget, curl,
<a href="http://metacpan.org/module/LWP::Simple">LWP::Simple</a>,
and <a href="http://metacpan.org/module/HTTP::Tiny">HTTP::Tiny</a>.

=abstract end

<h2>wget</h2>

While they are not Perl solutions, they can actually provide a quick solution for you.
I think there are virtually no Linux distributions that don't come with either <hl>wget</hl>
or <hl>curl</hl>. They are both command line tool that can download files via various protocols,
including HTTP and HTTPS.

You can use the <hl>system</hl> function of Perl to execute external program so you can write the
following:

<code lang="perl">
my $url = 'https://perlmaven.com/';
system "wget $url";
</code>

This will download the main page from the perlmaven.com domain and save it on the disk.
You can then read that file into a variable of your Perl program.

However there is another, more straight-forward way to get the remote file in a variable.
You can use the <hl>qx</hl> operator (what you might have seen as back-tick ``) instead of the
<hl>system</hl> function, and you can ask <hl>wget</hl> to print the downloaded file to
the standard output instead of saving to a file. As <hl>qx</hl> will capture and return the standard
output of the external command, this can provide a convenient way to download a page directly into
a variable:

<code lang="perl">
my $url = 'https://perlmaven.com/';
my $html = qx{wget --quiet --output-document=- $url};
</code>

<hl>--output-document</hl> can tell <hl>wget</hl> where to save the downloaded file.
As a special case, if you pass a dash <hl>-</hl> to it, <hl>wget</hl> will print the
downloaded file to the standard output.

<hl>--quiet</hl> tells <hl>wget</hl> to avoid any output other than the actual content.

<h2>curl</h2>

For <hl>curl</hl> the default behavior is to print to the standard output,
and the <hl>--silent</hl> flag can tell it to avoid any other output.

This is the solution with <hl>curl</hl>:

<code lang="perl">
my $url = 'https://perlmaven.com/';
my $html = qx{curl --silent $url};
</code>

The drawback in both cases it that you rely on external tools and you probably
have less control over those than over perl-based solutions.

<h2>Get one page using LWP::Simple</h2>

Probably the most well know perl module implementing a web client is <hl>LWP</hl> and its
sub-modules. <a href="http://metacpan.org/module/LWP::Simple">LWP::Simple</a> is a,
not surprisingly, simple interface to the library.

The code to use it is very simple. It exports a function called <hl>get</hl>
that fetch the content of a single URL:

<include file="examples/get_lwp_simple.pl">

This is really simple, but in case of failure you don't know what really happened.
You just get an empty document.

<h2>Get one page using HTTP::Tiny</h2>

For that <a href="http://metacpan.org/module/HTTP::Tiny">HTTP::Tiny</a> is much better even if the code is slightly longer:

<include file="examples/get_http_tiny.pl">

<hl>HTTP::Tiny</hl> is object oriented, hence you first call the constructor <hl>new</hl>. It returns an object and
on that object you can immediately call the <hl>get</hl> method.

It returns a hash with a number of interesting keys: <hl>success</hl> will
be <a href="/boolean-values-in-perl">true or false</a>, <hl>content</hl> will hold the
actual html content. <hl>status</hl> is the HTTP status-code (200 for success, 404 for not found, etc.).

Try printing it out using <hl>Data::Dumper</hl>. It is very useful!

<h2>A fuller example with HTTP::Tiny</h2>

<include file="examples/get_http_tiny_full.pl">

The first part of the output was generated by the <hl>while</hl>-loop on the headers
hash, then we used <hl>Data::Dumper</hl> to print out the whole hash.
Well, except of the content itself, that we deleted from the hash. It would have been
to much for this article and if you'd like to see the content,
you can just visit the main page of the <a href="/">Perl Maven</a> site.

<code>
content-type: text/html; charset=utf-8
set-cookie: dancer.session=8724695823418674906981871865731; path=/; HttpOnly
x-powered-by: Perl Dancer 1.3114
server: HTTP::Server::PSGI
server: Perl Dancer 1.3114
content-length: 21932
date: Fri, 19 Jul 2013 15:20:18 GMT

$VAR1 = {
          'protocol' => 'HTTP/1.0',
          'headers' => {
                         'content-type' => 'text/html; charset=utf-8',
                         'set-cookie' => 'dancer.session=8724695823418674906981871865731; path=/; HttpOnly',
                         'x-powered-by' => 'Perl Dancer 1.3114',
                         'server' => [
                                       'HTTP::Server::PSGI',
                                       'Perl Dancer 1.3114'
                                     ],
                         'content-length' => '21932',
                         'date' => 'Fri, 19 Jul 2013 15:20:18 GMT'
                       },
          'success' => 1,
          'reason' => 'OK',
          'url' => 'https://perlmaven.com.local:5000/',
          'status' => '200'
        };
</code>


<h2>Downloading many pages</h2>

Finally we arrived giving an example of downloading many pages using <hl>HTTP::Tiny</hl>.

<include file="examples/get_http_tiny_download.pl">

The code is, quite straight forward. We have a list of URLs in the <hl>@urls</hl> array.
An <hl>HTTP::Tiny</hl> object is created and assigned to the <hl>$ht</hl> variable. The
in a for-loop we go over each url and fetch it.

In order to save space in this article I only printed the size of each page.

This is the result:

<code>
Start https://perlmaven.com/
Length: 19959
Start https://cn.perlmaven.com/
Length: 13322
Start https://br.perlmaven.com/
Length: 12670
Start https://httpbin.org/status/404
Failed: 404 NOT FOUND
Start https://httpbin.org/status/509
Failed: 509 UNKNOWN
</code>

The simplicity has a price of course. It means that we wait for each request  to be finished
before we send out a new request. As most of the time is spent waiting for the the request to travel
to the remote server, then waiting for the remote server to process the request,
and then waiting till the response reaches us, we waste quite a lot of time. We could have sent all 3
requests in parallel and we would get our results much sooner.

However, this is going to be covered in another article.

